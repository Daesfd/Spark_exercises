{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc99308",
   "metadata": {},
   "source": [
    "# Como trabalhar com arquivos csv com numero de colunas variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d703a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ed09b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/03 21:26:54 WARN Utils: Your hostname, computador resolves to a loopback address: 127.0.1.1; using 10.0.0.135 instead (on interface wlp2s0)\n",
      "22/07/03 21:26:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/edson/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/03 21:26:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "        .config(\"spark.executor.memory\", \"500mb\") \\\n",
    "        .appName(\"Ex4\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa913179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/data.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b3e0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+----+\n",
      "| Id| Nome|    Local|Idade|Sexo|\n",
      "+---+-----+---------+-----+----+\n",
      "|  1|Pedro|     null| null|null|\n",
      "|  2| Joao|Sao Paulo| null|null|\n",
      "|  3|  Ana|     null| null|null|\n",
      "|  4|Maria|   Recife|   40|   f|\n",
      "|  5|Lucas| Campinas|   22|null|\n",
      "+---+-----+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dbede04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lê o arquivo como texto, pois ao ler como csv, há a leittura das colunas que a primeira linha participa (2 colunas)\n",
    "df_without_columns = spark.read.text('./data/data_without_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9b3d423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|           1,Pedro,|\n",
      "|  2,Joao,Sao Paulo,|\n",
      "|             3,Ana,|\n",
      "|4,Maria,Recife,40,f|\n",
      "|5,Lucas,Campinas,22|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_without_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad69368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria-se uma coluna contendo os valores da tabela original e, por fim, exclui-se a tabela original\n",
    "df_without_columns = df_without_columns.withColumn('splittable_col', split('value', ',').alias('splittable_col')).drop('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ce91895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[splittable_col: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_without_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "392fb680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      splittable_col|\n",
      "+--------------------+\n",
      "|        [1, Pedro, ]|\n",
      "|[2, Joao, Sao Pau...|\n",
      "|          [3, Ana, ]|\n",
      "|[4, Maria, Recife...|\n",
      "|[5, Lucas, Campin...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_without_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cab719a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria novas colunas a partir de informações da coluna 'splittable_col'\n",
    "for i in range(df_without_columns.select(max(size('splittable_col'))).collect()[0][0]):\n",
    "    df_without_columns = df_without_columns.withColumn('col' + str(i), df_without_columns['splittable_col'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "78be4c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+---------+----+----+\n",
      "|      splittable_col|col0| col1|     col2|col3|col4|\n",
      "+--------------------+----+-----+---------+----+----+\n",
      "|        [1, Pedro, ]|   1|Pedro|         |null|null|\n",
      "|[2, Joao, Sao Pau...|   2| Joao|Sao Paulo|    |null|\n",
      "|          [3, Ana, ]|   3|  Ana|         |null|null|\n",
      "|[4, Maria, Recife...|   4|Maria|   Recife|  40|   f|\n",
      "|[5, Lucas, Campin...|   5|Lucas| Campinas|  22|null|\n",
      "+--------------------+----+-----+---------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_without_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37b4252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclui-se a coluna 'splittable_col'\n",
    "df_final = df_without_columns.drop('splittable_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "29c2d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+----+----+\n",
      "|col0| col1|     col2|col3|col4|\n",
      "+----+-----+---------+----+----+\n",
      "|   1|Pedro|         |null|null|\n",
      "|   2| Joao|Sao Paulo|    |null|\n",
      "|   3|  Ana|         |null|null|\n",
      "|   4|Maria|   Recife|  40|   f|\n",
      "|   5|Lucas| Campinas|  22|null|\n",
      "+----+-----+---------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f421c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
